# TOI Data Processing Scripts

This directory contains scripts for processing and filtering data related to TESS Objects of Interest (TOI).

## Scripts

All scripts should be run from the project's root directory.

### 1. `split_data.py`

This script splits the source TOI CSV data into `train_set.csv` and `test_set.csv`. It ensures that all data points for a single star (tid) belong to the same set (either training or testing).

**Usage:**

```bash
# Run with default arguments
python split_data.py

# Or provide a specific input file and output directory
python split_data.py [source_csv_path] [output_dir_path]
```

**Defaults:**
- Source CSV: `data/TOI_2025.10.03_22.49.45.csv`
- Output Directory: `data`

### 2. `interactive_filter.py`

This script allows you to interactively select columns to keep from a CSV file. It will go through each column one by one, show you a summary, and ask if you want to keep it. The names of the columns you "pick" are saved to a text file.

**Usage:**
```bash
python interactive_filter.py [input_csv_path] [output_columns_file]
```

**Example:**
```bash
python interactive_filter.py data/train_set.csv data/selected_columns.txt
```

### 3. `filter_csv.py`

This script filters a CSV file, keeping only the columns specified in a column file (like the one generated by `interactive_filter.py`). It creates a new, smaller CSV file.

**Usage:**
```bash
python filter_csv.py [input_csv_path] [columns_file_path] [output_filtered_csv]
```

**Example:**
```bash
python filter_csv.py data/train_set.csv data/selected_columns.txt data/train_set_filtered.csv
```

### 4. `analyze_column.py`

This script provides a detailed analysis of each column in a CSV file. It shows the data type, missing value counts, and statistical summaries for numeric columns or value distributions for categorical columns.

**Usage:**

To analyze all columns:
```bash
python analyze_column.py [input_csv_path]
```

To analyze only the columns that contain missing values, use the `--missing-only` flag:
```bash
python analyze_column.py [input_csv_path] --missing-only
```

*Note: Initial analysis shows that the `st_dist`, `st_teff`, `st_logg`, and `st_rad` columns contain missing values.*

### 5. `score_model.py`

This script compares a prediction CSV file against a ground truth (GT) CSV file to evaluate model performance. It uses a JSON configuration file to determine which columns to use for scoring and how to map values to 'positive', 'negative', or 'dontcare'. It calculates and displays a confusion matrix, accuracy, precision, recall, and F1-score.

**Note:** This script is located in the project's root directory.

**Usage:**
```bash
# The config file is optional and defaults to 'scorer_conf.json'
python score_model.py <path_to_gt.csv> <path_to_pred.csv> [path_to_config.json]
```

**Example:**
```bash
# Compare a prediction file against the training set
python score_model.py data/train_set.csv data/train_set_pred.csv
```
A default `scorer_conf.json` is provided in the root directory. You can copy and modify it for different scoring rules.

## Example Workflow

```bash
# 1. Split the original dataset using default paths
python split_data.py

# 2. Interactively create a list of desired columns from the training set
python interactive_filter.py data/train_set.csv data/selected_columns.txt

# 3. Filter your train and test sets using the created list
python filter_csv.py data/train_set.csv data/selected_columns.txt data/train_set_filtered.csv
python filter_csv.py data/test_set.csv data/selected_columns.txt data/test_set_filtered.csv

# 4. Analyze the new filtered sets, for example to check for missing values
python analyze_column.py data/train_set_filtered.csv --missing-only
```

## Data Source

- **NASA Exoplanet Archive**: [TESS Objects of Interest (TOI) Table](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI)
